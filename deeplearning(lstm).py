# -*- coding: utf-8 -*-
"""DeepLearning(LSTM).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Ho5r12usYAsCqR96tnkZGaRo9XBCLSQ
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
import re
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import cross_val_score

# Specify the path to your Excel file
excel_file_path = '/content/drive/MyDrive/intellicruit/test3Dataset.xlsx'

# Read the Excel file into a DataFrame
df = pd.read_excel(excel_file_path)

# Set options to display all rows and columns
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)


df.head(4)

# Shuffle the DataFrame rows
df = df.sample(frac=1, random_state=42)

# Reset the index of the shuffled DataFrame
df = df.reset_index(drop=True)

nltk.download('stopwords')
stop_words = stopwords.words('english')

def remove_stopwords(content):
    con = re.sub('[^a-zA-Z]', ' ', content)
    con = con.lower()
    con = con.split()
    con = [word for word in con if not word in stopwords.words('english')]
    con = ' '.join(con)
    return con

df['Skill'] = df['Skill'].apply(remove_stopwords)

from sklearn.preprocessing import LabelEncoder
le_x= LabelEncoder()
df.Career = le_x.fit_transform(df.Career)

df.head(4)

unique_classes = le_x.classes_
for label, encoded_value in zip(unique_classes, range(len(unique_classes))):
    print(f"{label} is encoded as {encoded_value}")

import pandas as pd
import matplotlib.pyplot as plt

# Mapping encoded labels to their respective strings
labels = {
    0: 'Artificial Intelligence',
    1: 'Data Science',
    2: 'Development',
    3: 'Security',
    4: 'Software Development and Engineering',
    5: 'User Experience (UX) and User Interface (UI) Design'
}

# Count occurrences of each label
label_counts = df['Career'].value_counts()

# Calculate percentage for each label
percentages = (label_counts / len(df)) * 100

# Generate labels for the pie chart (with count and percentage)
labels_for_plot = [f"{labels[label]} - {label_counts[label]} ({percentages[label]:.1f}%)"
                   for label in label_counts.index]

# Plotting the pie chart
plt.figure(figsize=(8, 8))
plt.pie(label_counts, labels=labels_for_plot, autopct='', startangle=140)
plt.title('Percentage of Career Labels')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

# Mapping encoded labels to their respective strings
labels = {
    0: 'Artificial Intelligence',
    1: 'Data Science',
    2: 'Development',
    3: 'Security',
    4: 'Software Development and Engineering',
    5: 'User Experience (UX) and User Interface (UI) Design'
}

# Define the percentages for train-test split
train_percentage = 0.8
test_percentage = 0.2

# Empty DataFrames to store train and test splits
train_data = pd.DataFrame()
test_data = pd.DataFrame()

# Iterate through each class
for label, total_count in labels.items():
    # Filter rows for the current label
    subset = df[df['Career'] == label]

    # Perform train-test split for the subset
    train_subset, test_subset = train_test_split(subset, train_size=train_percentage, test_size=test_percentage)

    # Append train and test splits to respective DataFrames
    train_data = train_data.append(train_subset)
    test_data = test_data.append(test_subset)

# Shuffle the DataFrame rows
train_data = train_data.sample(frac=1, random_state=42)

# Reset the index of the shuffled DataFrame
train_data = train_data.reset_index(drop=True)

# Shuffle the DataFrame rows
test_data = test_data.sample(frac=1, random_state=42)

# Reset the index of the shuffled DataFrame
test_data = test_data.reset_index(drop=True)

x_train = train_data['Skill']
x_test = test_data['Skill']

y_train = train_data['Career']
y_test = test_data['Career']

vect=TfidfVectorizer()
x_train=vect.fit_transform(x_train)
x_test=vect.transform(x_test)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, LSTM, Dropout, GlobalMaxPooling1D, Reshape

from tensorflow.keras.layers import LSTM

# Reshape data for LSTM (assuming a 3D input)
x_train_all_lstm = x_train.toarray().reshape(x_train.shape[0], 1, x_train.shape[1])
x_test_all_lstm = x_test.toarray().reshape(x_test.shape[0], 1, x_test.shape[1])


x_train_all_lstm, x_val_all_lstm, y_train, y_val = train_test_split(x_train_all_lstm, y_train, test_size=0.2, random_state=42, stratify=y_train)

# Define batch size
batch_size_lstm = 32
# Define the LSTM model
lstm_model = Sequential()
lstm_model.add(LSTM(units=64, input_shape=(1, x_train_all_lstm.shape[2])))
lstm_model.add(Dropout(0.5))
lstm_model.add(Dense(64, activation='relu'))
lstm_model.add(Dropout(0.5))
num_classes_lstm = len(np.unique(y_train))
lstm_model.add(Dense(num_classes_lstm, activation='softmax'))

# Compile the LSTM model
lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the LSTM model
lstm_model.fit(x_train_all_lstm, y_train, epochs=50, batch_size=batch_size_lstm, validation_data=(x_val_all_lstm, y_val))

# Evaluate the LSTM model on the test set
test_loss_lstm, test_accuracy_lstm = lstm_model.evaluate(x_test_all_lstm, y_test)
print(f'LSTM Model Test Accuracy: {test_accuracy_lstm}')

import matplotlib.pyplot as plt

# Train the CNN model and record history
history = lstm_model.fit(x_train_all_lstm, y_train, epochs=50, batch_size=batch_size_lstm, validation_data=(x_val_all_lstm, y_val))

# Extract training and validation accuracy and loss from history
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']

# Create learning curves
epochs = range(1, len(train_accuracy) + 1)

# Plot training and validation accuracy
plt.plot(epochs, train_accuracy, 'b', label='Training Accuracy')
plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot training and validation loss
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Make predictions
predictions = lstm_model.predict(x_test_all_lstm)
predicted_labels = np.argmax(predictions, axis=1)

# Generate confusion matrix
cm = confusion_matrix(y_test, predicted_labels)
print(cm)

# Display the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Generate classification report
report = classification_report(y_test, predicted_labels)
print("Classification Report:\n", report)

# Compute precision, recall, and f1-score for each class
precision_lstm, recall_lstm, f1_lstm, support_lstm = precision_recall_fscore_support(y_test, predicted_labels, average=None)

# Compute weighted average
weighted_precision_lstm = np.sum(precision_lstm * support_lstm) / np.sum(support_lstm)
weighted_recall_lstm = np.sum(recall_lstm * support_lstm) / np.sum(support_lstm)
weighted_f1_lstm = np.sum(f1_lstm * support_lstm) / np.sum(support_lstm)

print(f"Weighted Precision for lstm: {weighted_precision_lstm}")
print(f"Weighted Recall for lstm: {weighted_recall_lstm}")
print(f"Weighted F1-Score for lstm: {weighted_f1_lstm}")

p_lstm = round(weighted_precision_lstm * 100, 2)
r_lstm = round(weighted_recall_lstm * 100, 2)
f1_lstm = round(weighted_f1_lstm * 100, 2)